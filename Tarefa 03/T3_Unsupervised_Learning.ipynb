{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dcyc-84ZxZ3X"
      },
      "source": [
        "# **Task \\#3**: Machine Learning MC886/MO444\n",
        "##**Unsupervised Learning and Dimension Reduction Techniques**##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V-bICd5xKLs",
        "outputId": "3d8cdb88-5eb5-44e5-b995-b4c9e0b4db58"
      },
      "outputs": [],
      "source": [
        "print('Marcelo Antunes Soares Fantini' + ' RA 108341')\n",
        "print('Rubens de Castro Pereira' + ' RA 217146') "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J1V9KDI-xoJ-"
      },
      "source": [
        "## Objective:\n",
        "\n",
        "Explore **Dimension Reduction Techniques** and **Unsupervised Algorithms** alternatives and come up with the best possible model for the problems."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1makPxBqx2Z5"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The MNIST dataset is a widely used benchmark dataset in the field of machine learning and computer vision. It consists of a collection of 70,000 handwritten digits, with each digit being a grayscale image of size 28x28 pixels. The digits range from 0 to 9 and are evenly distributed in the dataset.\n",
        "\n",
        "Dataset Information:\n",
        "\n",
        "- You should respect the following traininig/test split: 42,000 training examples, and 28,000 test examples.\n",
        "\n",
        "- Each training and test example is assigned to what number is in the sample.\n",
        "\n",
        "- Each row is a separate image. Column 1 is the class label. The remaining columns are pixel numbers (784 total). Each value is the darkness of the pixel (1 to 255).\n",
        "\n",
        "- The data is available at: ([Link of the Dataset](https://drive.google.com/drive/folders/13_nDDMrdIq2pCQU8kAnUQWBefpOAB-71?usp=sharing)): ```digit_recognizer_train.csv``` + ```digit_recognizer_test.csv```\n",
        "\n",
        "\n",
        "More information about the dataset: *Y. Lecun, L. Bottou, Y. Bengio and P. Haffner, \"Gradient-based learning applied to document recognition,\" in Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, Nov. 1998, doi: 10.1109/5.726791.https://ieeexplore.ieee.org/document/726791*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "C_UmKrLcxkjJ",
        "outputId": "8fb482b3-0ade-4206-cac6-bd078d5de8c4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patheffects as PathEffects\n",
        "from matplotlib import cm\n",
        "from matplotlib.ticker import LinearLocator\n",
        "\n",
        "import sklearn\n",
        "# from sklearn.datasets import load_digits\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.cluster import Birch\n",
        "from sklearn.cluster import BisectingKMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from umap import UMAP\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "seed = 217146\n",
        "\n",
        "print('Importing done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## ------ Reading Training Dataset from Google drive ----- ##\n",
        "train_url='https://drive.google.com/uc?export=download&confirm=9iBg&id=1U_HeEtr_IznK413FMgzIvjd1WHMU66nX'\n",
        "train_df = pd.read_csv(train_url)\n",
        "# train_df = pd.read_csv('data/digit_recognizer_train.csv')\n",
        "\n",
        "train_df = train_df.sample(frac=1)\n",
        "train_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## ------ Reading Test Dataset  ----- ##\n",
        "test_url='https://drive.google.com/uc?export=download&confirm=9iBg&id=11uD9mh9Ebvdb8b714BbAzNOgcd92oJPb'\n",
        "test_df = pd.read_csv(test_url)\n",
        "# test_df = pd.read_csv('data/digit_recognizer_test.csv')\n",
        "\n",
        "# extracting label from digits matrix \n",
        "test_labels = test_df['label']\n",
        "test_digits = test_df.loc[:, test_df.columns != 'label']\n",
        "\n",
        "# showing some lines os test dataset \n",
        "test_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "ftr6evRQz1-L",
        "outputId": "3c10efe2-183f-4fc6-aabf-7db1019bad4f"
      },
      "outputs": [],
      "source": [
        "## ------ Plot Data ----- ##\n",
        "\n",
        "fig, axes = plt.subplots(4, 10, figsize=(10,5), subplot_kw={'xticks':[], 'yticks':[]})\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(train_df.iloc[i][1:].values.reshape(28,28), cmap='binary', interpolation='nearest')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j46zvSwzz9oH"
      },
      "source": [
        "## 1. Dimensionality Reduction Techniques\n",
        "\n",
        "Analyze the distribution by plotting th embeddings generated with the aid of dimensionality reduction techniques.\n",
        "\n",
        "*Obs: Remember to normalize the data* and *test differents hyperparameters* "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0E5jKSmQz9o2"
      },
      "source": [
        "(0.5 point) Plot a 2D graph using the [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# extracting label from digits matrix \n",
        "labels = train_df['label']\n",
        "digits = train_df.loc[:, train_df.columns != 'label']\n",
        "\n",
        "# normlizing just the digits matrix \n",
        "scaler = StandardScaler()\n",
        "digits_scaled = scaler.fit_transform(digits)\n",
        "\n",
        "# print(f'digits_scaled: {digits_scaled}')\n",
        "# print(f'-'*50)\n",
        "print(f'Normalizing features (digits)')\n",
        "print()\n",
        "print(f'labels        : {labels.shape}')\n",
        "print(f'digits        : {digits.shape}')\n",
        "print(f'digits scaled : {digits_scaled.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Função para plotar os resultados, extraída de:\n",
        "\n",
        "#\n",
        "# https://www.oreilly.com/content/an-illustrated-introduction-to-the-t-sne-algorithm/\n",
        "#\n",
        "def scatter(x, colors):\n",
        "    # We choose a color palette with seaborn.\n",
        "    palette = np.array(sns.color_palette(\"hls\", 10))\n",
        "\n",
        "    # We create a scatter plot.\n",
        "    f = plt.figure(figsize=(8, 8))\n",
        "    ax = plt.subplot(aspect='equal')\n",
        "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
        "                    c=palette[colors.astype(np.int)])\n",
        "    plt.xlim(-25, 25)\n",
        "    plt.ylim(-25, 25)\n",
        "    ax.axis('off')\n",
        "    ax.axis('tight')\n",
        "\n",
        "    # We add the labels for each digit.\n",
        "    txts = []\n",
        "    for i in range(10):\n",
        "        # Position of each label.\n",
        "        xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
        "        txt = ax.text(xtext, ytext, str(i), fontsize=24)\n",
        "        txt.set_path_effects([\n",
        "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
        "            PathEffects.Normal()])\n",
        "        txts.append(txt)\n",
        "\n",
        "    return f, ax, sc, txts\n",
        "\n",
        "print(f'Compilou a função \"scatter()\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltCYyiD74mUq"
      },
      "outputs": [],
      "source": [
        "# TODO: Principal Component Analysis (PCA)\n",
        "\n",
        "number_of_component = 2\n",
        "model_pca = PCA(n_components=number_of_component, random_state=seed)\n",
        "digits_pca = model_pca.fit(digits_scaled).transform(digits_scaled)\n",
        "\n",
        "# show plot \n",
        "scatter(digits_pca, labels)\n",
        "plt.title('PCA com dados originais e os 2 primeiros componentes principais')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-qKXvlfL1DUe"
      },
      "source": [
        "(0.5 point) Plot a 2D graph using the [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QG6rO1uT4rSy"
      },
      "outputs": [],
      "source": [
        "# TODO: T-distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "\n",
        "# What, Why and How of t-SNE: https://towardsdatascience.com/what-why-and-how-of-t-sne-1f78d13e224d\n",
        "\n",
        "# model_tsne = TSNE(n_components=2, random_state=0, perplexity=50, n_iter=250)\n",
        "model_tsne = TSNE(n_components=2, random_state=seed, perplexity=30, n_jobs=-1)\n",
        "digits_tsne = model_tsne.fit_transform(digits_scaled)\n",
        " \n",
        "print(digits_scaled.shape) \n",
        "print(digits_tsne.shape) \n",
        "\n",
        "# show plot \n",
        "scatter(digits_tsne, labels)\n",
        "plt.title('t-SNE, 2 primeiros componentes principais')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "G-7s2uX-1DXC"
      },
      "source": [
        "(0.5 point) Plot a 2D graph using a combination of PCA + TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vMMnXPT5qQp"
      },
      "outputs": [],
      "source": [
        "# TODO: PCA + t-SNE\n",
        "\n",
        "# method PCA reducing from 784 to 50 features \n",
        "model_pca2 = PCA(n_components=50, random_state=seed)\n",
        "digits_pca2 = model_pca2.fit(digits_scaled).transform(digits_scaled)\n",
        "\n",
        "# method t-SNE reducing from 50 to 2 features \n",
        "model_tsne2 = TSNE(n_components=2, random_state=seed, perplexity=30, n_jobs=-1)\n",
        "digits_tsne2 = model_tsne.fit_transform(digits_pca2)\n",
        " \n",
        "print(digits_scaled.shape) \n",
        "print(digits_tsne.shape) \n",
        "\n",
        "# show plot \n",
        "scatter(digits_tsne2, labels)\n",
        "plt.title('PCA (Reducing from 784 to 50 components) + t-SNE (Reducing from 50 to 2 components)')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Otp68xcg1DZm"
      },
      "source": [
        "(0.25 point) Plot a 3D graph using the best representation founded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDSWTUzB09Aj"
      },
      "outputs": [],
      "source": [
        "# TODO: 3D Graph\n",
        "\n",
        "# setting number of components for PCA and t-SNE methods \n",
        "n_components_original = digits_scaled.shape[1]\n",
        "n_components_pca   = 50\n",
        "n_components_t_SNE = 3\n",
        "\n",
        "# method PCA reducing from 784 to 50 features \n",
        "n_components_pca = 50\n",
        "model_pca3 = PCA(n_components=n_components_pca, random_state=seed)\n",
        "digits_pca3 = model_pca3.fit(digits_scaled).transform(digits_scaled)\n",
        "\n",
        "# method t-SNE reducing from 50 to 3 features \n",
        "model_tsne3 = TSNE(n_components=n_components_t_SNE, random_state=seed, perplexity=30, n_jobs=-1)\n",
        "digits_tsne3 = model_tsne3.fit_transform(digits_pca3)\n",
        "\n",
        "print(digits_tsne3.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #######################################################################################################\n",
        "# plot 3D Graph \n",
        "# from https://www.tutorialspoint.com/creating-a-3d-plot-in-matplotlib-from-a-3d-numpy-array\n",
        "# #######################################################################################################\n",
        "\n",
        "# We choose a color palette with seaborn.\n",
        "palette = np.array(sns.color_palette(\"hls\", 10))\n",
        "labels_colors = palette[labels.astype(np.int_)]\n",
        "\n",
        "# Make data.\n",
        "X = digits_tsne3[:,0]\n",
        "Y = digits_tsne3[:,1]\n",
        "Z = digits_tsne3[:,2]\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = [13.00, 9.00]\n",
        "plt.rcParams[\"figure.autolayout\"] = True\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter3D(X, Y, Z, c=labels_colors)\n",
        "ax.set_xlabel('Componente 1 ( X )')\n",
        "ax.set_ylabel('Componente 2 ( Y )')\n",
        "ax.set_zlabel('Componente 3 ( Z )')\n",
        "title = f'PCA (Reducing from {n_components_original} to {n_components_pca} components ' + \\\n",
        "        f't-SNE (Reducing from {n_components_pca} to {n_components_t_SNE} components'\n",
        "\n",
        "plt.title('PCA (784 to 50 components) + t-SNE (50 to 3 components)')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "g5fZ8fJA6mD-"
      },
      "source": [
        "**(1.75 point) Questions:**\n",
        "1. What is the best representation, and why?\n",
        "2. Do the combination of techniques help improve the results?\n",
        "3. Did adding another dimension help enhance the performance?\n",
        "4. How can we use this knowledge to decide which model to train?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answers of Dimensionality Reduction Techniques (Section 1)**\n",
        "\n",
        "1. The best representation is the combination of PCA to reduce the number of components to a smaller number, in this case we chose 50, and then use t-SNE to further reduce to two components only. The PCA reduced the number of components to a manageable size which enabled the optimal use of t-SNE.\n",
        "\n",
        "2. Yes, the combination of the techniques has greatly improved the results, as seen in the 2D graph obtained. The clusters are well-defined in comparison to the graphs where PCA or t-SNE were used individually.\n",
        "\n",
        "3. It did not enhance performance, the third dimension has worsened it. It took more than three times the time to output the 3D graph as it took to plot the 2D graph. You can't visualize all clusters because of the 3D nature of the graph.\n",
        "\n",
        "4. We can use the best 2D model, which is the combination of PCA and t-SNE, which has the best performance and best clustering from all the models considered."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WwcXwamBczG4"
      },
      "source": [
        "(Optional) Additionally, you can visually explore the data and its distribution by plotting the original image in a 2D space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLT-CXJqctVx"
      },
      "outputs": [],
      "source": [
        "def plot_images(best_embedding, images, width=2000, height=1000, max_dim=10):\n",
        "  '''\n",
        "    best_embedding : np.array\n",
        "      Best representation found\n",
        "\n",
        "    images : np.array\n",
        "      Original Images  \n",
        "  '''\n",
        "  \n",
        "  tx, ty = best_embedding[:, 0], best_embedding[:, 1]\n",
        "  tx = (tx-np.min(tx)) / (np.max(tx) - np.min(tx))\n",
        "  ty = (ty-np.min(ty)) / (np.max(ty) - np.min(ty))\n",
        "\n",
        "  full_image = Image.new('RGBA', (width, height))\n",
        "\n",
        "  for data, x, y in zip(images, tx, ty):\n",
        "    tile = Image.fromarray(np.uint8(data.reshape(28,28)))\n",
        "    full_image.paste(tile, (int((width-max_dim)*x), int((height-max_dim)*y)), mask=tile.convert('RGBA'))\n",
        "\n",
        "  plt.figure(figsize = (16,12))\n",
        "  plt.imshow(full_image)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZTh3ZO6c9x5"
      },
      "outputs": [],
      "source": [
        "# Plot the original images using two arrays: \n",
        "#    best_embedding with the components (n_components=2) from either PCA or t-SNE;\n",
        "#    images with the original data. Both arrays should correspond to the same data.\n",
        "\n",
        "plot_images(digits_tsne2, train_df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "79qT-6f9C262"
      },
      "source": [
        "(Optional) Besides this algorithms, another popular algorithm is Uniform Manifold Approximation and Projection ([UMAP](https://umap-learn.readthedocs.io/en/latest/)). Plot a 2D graph using this technique.\n",
        "\n",
        "Obs: *here is a great video that explains this method and the differences between it and t-SNE*. \n",
        "\n",
        "Link: https://www.youtube.com/watch?v=eN0wFzBA4Sc&ab_channel=StatQuestwithJoshStarmer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkBy_Wm3ZiYn"
      },
      "outputs": [],
      "source": [
        "# !pip install umap-learn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-w0Bnv4GFMn"
      },
      "outputs": [],
      "source": [
        "# Uniform Manifold Approximation and Projection (UMAP)\n",
        "\n",
        "# getting instance of UMAP \n",
        "reducer = UMAP()\n",
        "\n",
        "# training and reducing scaled data \n",
        "embedding  = reducer.fit_transform(digits_scaled)\n",
        "\n",
        "# plotting result \n",
        "scatter(embedding, labels)\n",
        "plt.title('UMAP projection of the digits dataset', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xXT7kwVW6xbe"
      },
      "source": [
        "## 2. Unsupervised Learning\n",
        "\n",
        "The main idea of this section is to train an unsupervised learning algorithm to identify the possible groups present in the MNIST dataset. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K0MlXdJMVH_X"
      },
      "source": [
        "(0.25 point) Train a KMeans algorithm using the best representation found in the previous section and plot the resulting clusters. Compare with the original representation.\n",
        "\n",
        "*Obs: Use ``` n_cluster = 10 ``` for the KMeans algorithm.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QC36gU3qt81"
      },
      "outputs": [],
      "source": [
        "# TODO: KMeans with a dimension reduction technique.\n",
        "\n",
        "# getting KMeans object with its hyperparameters \n",
        "# kmeans = KMeans(n_clusters=10, n_init=50, max_iter=300, init='random')\n",
        "kmeans_model = KMeans(n_clusters=10, n_init=50)\n",
        "\n",
        "# fitting components \n",
        "kmeans_model.fit(digits_tsne2)\n",
        "\n",
        "print(f'kmeans.cluster_centers_ {kmeans_model.cluster_centers_}')\n",
        "print()\n",
        "print(f'kmeans.labels_ {kmeans_model.labels_}')\n",
        "\n",
        "# show plot \n",
        "plt.scatter(digits_tsne2[:,0],digits_tsne2[:,1], c=kmeans_model.labels_, cmap='rainbow')\n",
        "\n",
        "plt.title('KMeans utilizando dados com redução de dimensionalidade')\n",
        "plt.xlabel('Componente 1')\n",
        "plt.ylabel('Componente 2')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GM7V3_zKgw1Z"
      },
      "source": [
        "(0.5 points) Choose the best ```n_cluster``` using the Elbow Method. Plot the graph to assist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ejap-asTtDaw"
      },
      "outputs": [],
      "source": [
        "# TODO: Elbow Method and 2D plot\n",
        "\n",
        "# Elbow Method and 2D plot\n",
        "# From https://predictivehacks.com/k-means-elbow-method-code-for-python/\n",
        "\n",
        "distortions = []\n",
        "K = range(1,11)\n",
        "for k in K:\n",
        "    kmeans_model = KMeans(n_clusters=k, n_init=50)\n",
        "    kmeans_model.fit(digits_tsne2)\n",
        "\n",
        "    distortions.append(kmeans_model.inertia_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show plot \n",
        "\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.plot(K, distortions, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Distortion')\n",
        "plt.title('The Elbow Method showing the optimal k')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GKN3qVNbg4hD"
      },
      "source": [
        "(0.5 points) Choose the best ```n_cluster``` using the Silhouette Analysis. Plot the silhouette and the average score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0rKk3Za9YCi"
      },
      "outputs": [],
      "source": [
        "# TODO: Silhouette Method -- Average Score and plot\n",
        "\n",
        "# Silhouette Method -- Average Score and plot\n",
        "# From https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
        "\n",
        "\n",
        "range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(digits_tsne2) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, n_init=\"auto\", random_state=seed)\n",
        "    cluster_labels = clusterer.fit_predict(digits_tsne2)\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(digits_tsne2, cluster_labels)\n",
        "    print(\n",
        "        \"For n_clusters =\",\n",
        "        n_clusters,\n",
        "        \"The average silhouette_score is :\",\n",
        "        silhouette_avg,\n",
        "    )\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(digits_tsne2, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(\n",
        "            np.arange(y_lower, y_upper),\n",
        "            0,\n",
        "            ith_cluster_silhouette_values,\n",
        "            facecolor=color,\n",
        "            edgecolor=color,\n",
        "            alpha=0.7,\n",
        "        )\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "    ax2.scatter(\n",
        "        digits_tsne2[:, 0], digits_tsne2[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
        "    )\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(\n",
        "        centers[:, 0],\n",
        "        centers[:, 1],\n",
        "        marker=\"o\",\n",
        "        c=\"white\",\n",
        "        alpha=1,\n",
        "        s=200,\n",
        "        edgecolor=\"k\",\n",
        "    )\n",
        "\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
        "\n",
        "    ax2.set_title(\"The visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "\n",
        "    plt.suptitle(\n",
        "        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
        "        % n_clusters,\n",
        "        fontsize=14,\n",
        "        fontweight=\"bold\",\n",
        "    )\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "y_ESW9tQW2cr"
      },
      "source": [
        "(0.25 points) Train a different clustering algorithm from the [scikit-learn library](https://scikit-learn.org/stable/modules/clustering.html) and compare its results with those of KMeans and the original data distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqBJaK50W1xk"
      },
      "outputs": [],
      "source": [
        "# TODO: Unsupervised Algorithm of your choose\n",
        "\n",
        "# Birch method selected from scikit-learn clusters \n",
        "# From https://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html\n",
        "\n",
        "brc = Birch(n_clusters=10)\n",
        "brc.fit(digits_tsne2)\n",
        "cluster_labels = brc.predict(digits_tsne2)\n",
        "\n",
        "# show plot \n",
        "plt.scatter(digits_tsne2[:,0],digits_tsne2[:,1], c=cluster_labels, cmap='rainbow')\n",
        "\n",
        "plt.title('Birch utilizando dados com redução de dimensionalidade')\n",
        "plt.xlabel('Componente 1')\n",
        "plt.ylabel('Componente 2')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bisecting KMeans method selected from scikit-learn clusters \n",
        "# From https://scikit-learn.org/stable/modules/generated/sklearn.cluster.BisectingKMeans.html#sklearn.cluster.BisectingKMeans\n",
        "\n",
        "bisect_means = BisectingKMeans(n_clusters=10, random_state=seed)\n",
        "bisect_means.fit(digits_tsne2)\n",
        "cluster_labels = bisect_means.predict(digits_tsne2)\n",
        "\n",
        "# show plot \n",
        "plt.scatter(digits_tsne2[:,0],digits_tsne2[:,1], c=cluster_labels, cmap='rainbow')\n",
        "\n",
        "plt.title('Bisecting KMeans utilizando dados com redução de dimensionalidade')\n",
        "plt.xlabel('Componente 1')\n",
        "plt.ylabel('Componente 2')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cDiiMI67B9H5"
      },
      "source": [
        "**(2 points) Questions:**\n",
        "\n",
        "1. Did KMeans provide good separation (clusters) when compared to the original distribution? Why?\n",
        "2. Did the Elbow and Silhouette methods suggest the same number of clusters? Did these techniques suggest the same n_cluster as the original dataset?\n",
        "3. If you did not know the number of classes in the MNIST dataset, which method would you use and why? Is the suggested number of clusters the same as the number of classes in the dataset? Why do you think it is the same or different?\n",
        "4. When compared to other algorithms, did KMeans perform better? How can different clustering techniques be compared?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Answers\n",
        "\n",
        "1. Yes, the KMeans did provide good clusters when compared to the original distribution since they are better separated with more neatly delineated clusters as compared to the original. It's probably because KMeans works by computing the distance between points and the cluster centers, resulting in misclassification of points that are too far but ending up with nicer clusters.\n",
        "\n",
        "2. The Elbow method and the Silhouette analysis did not suggest the same number of clusters. The Elbow method suggested 4 clusters. To determine the number of clusters using the Silhouette analysis we observed the mean silhouette score in the plot, the height of each knife in the plot and the number of points with negative score in the plot. With this in mind, the analysis suggested $n=6$ clusters. None of them suggested the same number of clusters as the original dataset.\n",
        "\n",
        "3. The Silhouette analysis would be our choice for this problem because the silhouette score and plot takes in account more characteristics than just the distance between clusters. The Elbow method uses only the Euclidean distance between points while Silhouette uses more nuanced measures. The number of clusters suggested by Silhouette analysis is different from the number of classes in the dataset. They are not the same because points are grouped by other measures rather than by features, thus leading the algorithm to assume there are fewer clusters than there actually are.\n",
        "\n",
        "4. We visually compared KMeans with Birch and Bisecting KMeans methods. The best of the three is the Birch method, followed by KMeans, and last the Bisecting KMeans method. The Birch method created neatly separated clusters, while KMeans divided these clusters more haphazardly. Some ways to compare different clustering techniques are: use a gold standard and compute the distance/similarity between clusterings; compare the silhouette score for each clustering technique; use simulations to compare the performance; compare the computational efficiency when using large datasets; and consider the assumptions and limitations for each clustering technique."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yRXUEyMgl8Y7"
      },
      "source": [
        "## 3. Classification with PCA\n",
        "\n",
        "PCA can be used in combination with a supervised learning model."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dzOj09ChiLLw"
      },
      "source": [
        "(0.25 points) Baseline Model: Understanding the machine learning pipeline, explore a supervised model with the MNIST dataset.\n",
        "\n",
        "obs: *Remember to split and normalize the data*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaUWCRe3DojJ"
      },
      "outputs": [],
      "source": [
        "# TODO: Baseline Model\n",
        "\n",
        "# Spliting training dataset \n",
        "X_train_original, X_valid_original, y_train_original, y_valid_original = \\\n",
        "    train_test_split(digits, labels, train_size=0.76, test_size=0.24, random_state=seed)\n",
        "\n",
        "# setting test dataset \n",
        "X_test_original = test_digits\n",
        "y_test_original = test_labels\n",
        "\n",
        "# showing statistics\n",
        "dataset_total_size = len(digits) + len(test_digits)\n",
        "\n",
        "print(f'-'*70)\n",
        "print()\n",
        "print(f'Statistic of splitted dataset:')\n",
        "print()\n",
        "print(f'Training size   : {len(X_train_original)} - {(len(X_train_original) / dataset_total_size )*100:.2f}%')\n",
        "print(f'Validation size : {len(X_valid_original)} - {(len(X_valid_original) / dataset_total_size)*100:.2f}%')\n",
        "print(f'Testing size    : {len(X_test_original)} - {(len(X_test_original) / dataset_total_size)*100:.2f}%')\n",
        "print(f'Dataset size    : {dataset_total_size} - 100%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalizing features dataset \n",
        "scaler = StandardScaler()\n",
        "\n",
        "scaler.fit(X_train_original)\n",
        "\n",
        "X_train_scaled = scaler.transform(X_train_original)\n",
        "X_valid_scaled = scaler.transform(X_valid_original)\n",
        "X_test_scaled  = scaler.transform(X_test_original)\n",
        "\n",
        "print(f'The features train, valid, and test were normalized')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y, y_pred):\n",
        "    \"\"\"Plots a confusion matrix using seaborn comparing true values in y array with predicted values in y_pred array.\n",
        "\n",
        "    Args:\n",
        "        y (numpy.ndarray): Array with real values from set.\n",
        "        y_pred (numpy.ndarray): Array with predicted values from set.\n",
        "    \"\"\"\n",
        "    # transform y to numpy array\n",
        "    y = np.asarray(y).ravel()\n",
        "\n",
        "    # create dictionary and dataframe\n",
        "    data = {'y' : y, 'y_pred' : y_pred}\n",
        "    df = pd.DataFrame(data, columns=['y', 'y_pred'])\n",
        "\n",
        "    # create cross tabulation in pandas\n",
        "    color_map = pd.crosstab(df['y'], df['y_pred'], rownames=['Actual'], colnames=['Predicted'])\n",
        "    \n",
        "    # show heatmap\n",
        "    sns.heatmap(color_map, annot=True)\n",
        "    plt.show()\n",
        "\n",
        "print(f'Validation of \"plot_confusion_matrix()\" method with success.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'Logistic Regression used as Baseline Model')\n",
        "print()\n",
        "\n",
        "# Initializing model\n",
        "logistic_regression = LogisticRegression(random_state=seed, max_iter=1000)\n",
        "\n",
        "# fitting model \n",
        "logistic_regression.fit(X_train_scaled, y_train_original)\n",
        "\n",
        "# predicting results \n",
        "y_pred = logistic_regression.predict(X_valid_scaled)\n",
        "\n",
        "# calculating accuracy score\n",
        "baseline_accuracy_score = accuracy_score(y_valid_original, y_pred)\n",
        "print('accuracy: ', baseline_accuracy_score)\n",
        "\n",
        "# confusion matrix and f1 score\n",
        "baseline_f1_score = f1_score(y_valid_original, y_pred, average='macro')\n",
        "print('F1 score: ', baseline_f1_score)\n",
        "plot_confusion_matrix(y_valid_original, y_pred)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R_EHMRRfidga"
      },
      "source": [
        "(0.5 points) PCA model: Redo the experiment with the inclusion of PCA. Plot the accuracy (or other classification metric) against ```n_components``` and analyze the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdWCa3L7eJFI"
      },
      "outputs": [],
      "source": [
        "# TODO: Baseline Model with PCA\n",
        "\n",
        "# Reducing dimensionalitty of dataset according component parameter using PCA method \n",
        "\n",
        "def reduce_dimensionality(X_train, X_valid, X_test, number_of_components):\n",
        "\n",
        "    # fit model PCA with training data \n",
        "    model_pca = PCA(n_components=number_of_components, random_state=seed)\n",
        "    model_pca.fit(X_train_scaled)\n",
        "\n",
        "    # transforming train, valid and test features \n",
        "    X_train_PCA_components = model_pca.transform(X_train_scaled)\n",
        "    X_valid_PCA_components = model_pca.transform(X_valid_scaled)\n",
        "    X_test_PCA_components  = model_pca.transform(X_test_scaled)\n",
        "\n",
        "    print(f'-'*50)\n",
        "    print(f'Reducing dimensionalitty to {number_of_components} components using PCA')\n",
        "    print(X_train_PCA_components.shape)\n",
        "    print(X_valid_PCA_components.shape)\n",
        "    print(X_test_PCA_components.shape)\n",
        "    print()\n",
        "\n",
        "    # returnning data with its dimensionality reduced  \n",
        "    return X_train_PCA_components, X_valid_PCA_components, X_test_PCA_components\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plotar n_componentes x acurácia >> sugestão anotada durante atendiment5o de monitoria\n",
        " \n",
        "# Performing Logistic Regression with two features reduced with PCA\n",
        "\n",
        "# number of components \n",
        "components = [2, 4, 8, 16, 32, 64, 128, 256]\n",
        "\n",
        "# initializing list of f1-score and accuracy values  \n",
        "accuracy_values = []\n",
        "f1_score_values = []\n",
        "\n",
        "# running logistic regression with many n_components values\n",
        "for number_of_component in components:\n",
        "    print(f'-'*50)\n",
        "    print(f'Reducing dimensionalitty to {number_of_component} components')\n",
        "    print(f'-'*50)\n",
        "    print()\n",
        "\n",
        "    # getting dataset with reduced components\n",
        "    X_train_reduced, X_valid_reduced, X_test_reduced = \\\n",
        "        reduce_dimensionality(X_train_scaled, X_valid_scaled, X_test_scaled, number_of_component)\n",
        "\n",
        "    # Initializing model\n",
        "    logistic_regression = LogisticRegression(random_state=seed, max_iter=1000)\n",
        "\n",
        "    # fitting model \n",
        "    logistic_regression.fit(X_train_reduced, y_train_original)\n",
        "\n",
        "    # predicting results \n",
        "    y_pred_with_pca = logistic_regression.predict(X_valid_reduced)\n",
        "\n",
        "    # calculating accuracy score\n",
        "    baseline_accuracy_score = accuracy_score(y_valid_original, y_pred_with_pca)\n",
        "    print('accuracy: ', baseline_accuracy_score)\n",
        "\n",
        "    # confusion matrix and f1 score\n",
        "    baseline_f1_score = f1_score(y_valid_original, y_pred_with_pca, average='macro')\n",
        "    print('F1 score: ', baseline_f1_score)\n",
        "    plot_confusion_matrix(y_valid_original, y_pred_with_pca)\n",
        "\n",
        "    # initializing list of f1-score values and accuracy \n",
        "    f1_score_values.append(baseline_f1_score)\n",
        "    accuracy_values.append(baseline_accuracy_score)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'components : {components}')\n",
        "print(f'accuracy   : {accuracy_values}')\n",
        "print(f'f1-score   : {f1_score_values}')\n",
        "\n",
        "# plot graph\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(components, accuracy_values, linewidth=2.0, label='Accuracy')\n",
        "ax.plot(components, f1_score_values, linewidth=2.0, color='red', label='F1 Score')\n",
        "plt.title('Metrics Accuracy and F1 Score')\n",
        "plt.legend()\n",
        "plt.xticks(components)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jK_u97WiFnZV"
      },
      "source": [
        "(0.25 points) Evaluate both the baseline model and the PCA model using the test set. Display the confusion matrices for both cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1g9R0LV1FmxX"
      },
      "outputs": [],
      "source": [
        "# TODO: Evaluate models in Test split\n",
        "\n",
        "# Evaluating Baseline Logistic Regression in the test dataset\n",
        "print(f'Baseline Model using Logistic Regression in the Test dataset')\n",
        "print()\n",
        "\n",
        "# Initializing model\n",
        "test_logistic_regression = LogisticRegression(random_state=seed, max_iter=1000)\n",
        "\n",
        "# fitting model \n",
        "test_logistic_regression.fit(X_train_scaled, y_train_original)\n",
        "\n",
        "# predicting results \n",
        "y_pred_test = test_logistic_regression.predict(X_test_original)\n",
        "\n",
        "# calculating accuracy score\n",
        "test_accuracy_score = accuracy_score(y_test_original, y_pred_test)\n",
        "\n",
        "# confusion matrix and f1 score\n",
        "test_f1_score = f1_score(y_test_original, y_pred_test, average='macro')\n",
        "\n",
        "# print results \n",
        "print(f'Baseline Model (Logistic Regression) used by test dataset')\n",
        "print('Accuracy: ', test_accuracy_score)\n",
        "print('F1 score: ', test_f1_score)\n",
        "plot_confusion_matrix(y_test_original, y_pred_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluating Logistic Regression combined with PCA in the test dataset\n",
        "\n",
        "print(f'Logistic Regression combined with PCA in the Test dataset')\n",
        "print()\n",
        "\n",
        "best_number_of_components = 32\n",
        "\n",
        "# getting dataset with reduced components\n",
        "X_train_reduced, X_valid_reduced, X_test_reduced = \\\n",
        "    reduce_dimensionality(X_train_scaled, X_valid_scaled, X_test_scaled, best_number_of_components)\n",
        "\n",
        "# Initializing model\n",
        "test_pca_logistic_regression = LogisticRegression(random_state=seed, max_iter=1000)\n",
        "\n",
        "# fitting model \n",
        "test_pca_logistic_regression.fit(X_train_reduced, y_train_original)\n",
        "\n",
        "# predicting results \n",
        "y_pred_test_pca = test_pca_logistic_regression.predict(X_test_reduced)\n",
        "\n",
        "# calculating accuracy score\n",
        "test_pca_accuracy_score = accuracy_score(y_test_original, y_pred_test_pca)\n",
        "\n",
        "# confusion matrix and f1 score\n",
        "test_pca_f1_score = f1_score(y_test_original, y_pred_test_pca, average='macro')\n",
        "\n",
        "# print results \n",
        "print(f'Baseline Model (Logistic Regression) combined with PCA used by test dataset')\n",
        "print('Accuracy: ', test_pca_accuracy_score)\n",
        "print('F1 score: ', test_pca_f1_score)\n",
        "plot_confusion_matrix(y_test_original, y_pred_test_pca)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ArwRY8ZkFWlb"
      },
      "source": [
        "**(2 points) Questions:**\n",
        "\n",
        "1. Which model was used? What was the best result achieved without using PCA?\n",
        "2. What are your conclusions about the Baseline model?\n",
        "3. How did you define the best model for both approaches?\n",
        "4. Did the accuracy improve when using the PCA model?\n",
        "5. What are the advantages and disadvantages of using PCA on this dataset?\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Answers\n",
        "\n",
        "1. We used the Logistic Regression model as baseline model. The result of this model presented an accuracy of 0.9082 and  F1 score of 0.9069.\n",
        "\n",
        "2. The baseline model had good performance in terms of accuracy and f1 score, as mentioned in the previous answer. Additionally, the computational time was acceptable.\n",
        "\n",
        "3. The best model was defined by plotting accuracy and f1 score metrics as functions of the number of PCA components used. We chose the best number of components and compared these metrics to the baseline model metrics.\n",
        "\n",
        "4. The accuracy improved when using the PCA combined with the baseline model. Using the training dataset we tested different number of components for PCA and settled on 32 components. Applying PCA combined with the baseline model this yielded better accuracy and f1 scores on the test dataset.\n",
        "\n",
        "5. The advantages are reduced number of components to analyze, reducing computational time and complexity when applying model since the dataset has 784 features, while yielding better results on accuracy and f1 scores on the test dataset. The disadvantages are an increased number of steps in fine tuning the model to find the optimal tradeoff between number of components and loss of information, and more computational time spent on training."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8z78vzLrNilW"
      },
      "source": [
        "## Deadline\n",
        "\n",
        "Wednesday, May 22, 11:59 pm. \n",
        "\n",
        "Penalty policy for late submission: You are not encouraged to submit your assignment after due date. However, in case you do, your grade will be penalized as follows:\n",
        "- May 23, 11:59 pm : grade * 0.75\n",
        "- May 24, 11:59 pm : grade * 0.5\n",
        "- May 25, 11:59 pm : grade * 0.25\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wlKyVcM5NvKz"
      },
      "source": [
        "## Submission\n",
        "\n",
        "On Google Classroom, submit your Jupyter Notebook (in Portuguese or English).\n",
        "\n",
        "**This activity is NOT individual, it must be done in pairs (two-person group).**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
