\begin{enumerate}\setcounter{enumi}{2}\bfseries
    \item  \textbf{Apresente um artigo científico recente de uma solução para um problema de Ética em IA.}
\end{enumerate}

O artigo \textit{A Pathway Towards Responsible AI Generated Content} tem como propósito apresentar 
os riscos decorrentes do uso de modelos de Geração de Conteúdo por Inteligência Artificial (GCIA), 
principalmente aqueles voltados para a produção de conteúdo artístico. 
Ele discute e apresenta possíveis ações para o uso responsável, seguro, e ético desses modelos na 
sociedade \cite{chen2023pathway}.
Os tipos de conteúdos considerados nesses modelos são bastante variados, podendo ser imagens, textos, 
áudios ou vídeos. A maioria dos geradores de textos tem como base GPT
(\textit{Generative Pre-trained Transformers}) e suas versões evoluídas GPT-2 e GPT-3. 
Existem também os geradores de imagens a partir de textos (\textit{text-to-image}), tendo  
como base CLIP e OpenClip. Destacam-se 
recentemente o DALL-E e DALL-E 2, desenvolvidos pela OpenAI, e o Stable Diffusion, desenvolvido pela 
Stability AI.

O uso extensivo desses modelos traz preocupações muito relevantes. Podem afetar a privacidade e
gerar preconceito, informações tóxicas, desinformações, desrespeitar propriedade intelectual e 
existem potenciais usos indevidos por empresas ou pessoas. 

Recentemente surgiram discussões com a disponibilização de novas funcionalidades do ChatGPT, desenvolvido 
pelo OpenAI, permitindo depurar códigos fonte de programas de 
computador ou elaborar trabalhos escolares e acadêmicos. Surgem potenciais riscos com os produtos dessas funcionalidades, 
pois os modelos respondem conforme foram treinados, replicando conteúdos.
O conjunto de dados utilizados para treinamento frequentemente sem atribuição de origem e direitos autorais e sem curadoria cuidadosa. 
Além disso, a maioria dos modelos de GCIA decodificadores de textos são treinados com grandes quantidades 
de dados obtidos da internet. Consequentemente, eles podem conter desvios e viéses (\textit{biases}) relacionados a temas sociais, toxicidade, 
e outros riscos inerentes aos grandes modelos de linguagens.

Para que os modelos de GCIA sejam considerados responsáveis deve-se incluir nas considerações éticas: 
privacidade, viéses (tendências), toxicidade, desinformação, e proteção da propriedade intelectual. 
Adicionalmente, também devem contemplar robustez, possibilitar explicações dos resultados,
oferecer código fonte aberto, permitir consentimento dos autores para uso nos modelos, 
respeitar créditos autorais dos resultados, oferecer compensação aos proprietários dos dados quando 
estes são utilizados nos modelos e, por fim, possibilitar um ambiente amigável para o seu uso.

Os modelos geradores de conteúdo permitem uma vulnerabilidade de ataques de privacidade devido ao grande 
volume de dados duplicados nos conjuntos de dados de treinamento. 
Esse comportamento de replicação tem sido extensivamente estudado nestes modelos e resultar em imagens 
como a combinação de fundo e de objetos de imagens reais. Um exemplo desse resultado 
ocorreu com a Stable Diffusion, em que a imagem  final era a combinação simples de imagens do dataset de treinamento 
considerando o plano de fundo (\textit{background}) e plano da frente (\textit{foreground}). Devido a esse comportamento de 
replicação esses resultados divulgam imagens particulares de seus autores reais como se fossem 
imagens do próprio modelo, demonstrando a memorização dos modelos ao longo
do treinamento, que acaba reproduzindo, e não criando, novas imagens.

As questões de privacidade ainda não possuem soluções definitivas, mas ações vem sendo tomadas para minimizar essas consequências danosas. As companhias tem disponibilizado um website para fornecer identificação de imagens já treinadas como a companhia de arte Spawning AI. 
A empresa OpenAI segue nessa linha, reconhecendo as dificuldades para eliminar dados duplicados. 
Outras companhias como Microsoft e Amazon têm adotado medidas para prevenir o compartilhamento de 
dados sensíveis pelos seus empregados, evitando o uso desses dados em futuras versões de modelos de GCIA.
Atualmente as medidas para evitar o vazamento de dados privados são insuficientes e 
ainda faz-se necessário explorar sistemas confiáveis para a detecção de dados duplicados em modelos generativos e uma maior investigação no processo de memorização e generalização em sistemas de aprendizado profundo.

Os dados de treinamento utilizados nos modelos de Inteligência Artificial (IA) são obtidos do mundo real, podendo reforçar estereótipos indesejados, excluir ou marginalizar grupos de indivíduos, exacerbar toxicidade no discurso e podendo levar à incitação ao ódio e violência.
Um exemplo é o conjunto de dados LAION (\textit{Large-scale Artificial Intelligence Open Network}), contendo conteúdos relacionados à estereotipagem social,
pornografia, calúnias racistas e violência. O uso de filtros nos dados é uma possibilidade para minimizar esses problemas, mas podem introduzir viéses nos dados de treinamento que serão propagados pelos modelos.


Para ilustrar modelos que podem produzir resultados tendenciosos, a Figura \ref{fig:images_of_three_engineers} apresenta 
imagens geradas por meio do comando \textit{prompt} ``\textit{Three engineers running on the grassland}''. 
O resultado foram indivíduos do sexo masculino e brancos, excluindo indivíduos de minorias raciais e
indicando uma ausência de diversidade de gênero.
Outras estratégias para minimizar esses viéses são o emprego de técnicas de pré-treinamento aplicadas aos dados antes do treinamento do 
modelo e o contínuo treinamento com as informações mais recentes. Assim, evita-se o 
surgimento de lacunas de informação e garante-se que os modelos permaneçam atualizados, relevantes, 
e benéficos para a sociedade. Existem oportunidades desafiadoras para investigação mais profunda de viéses,
toxicidade e desinformação em todo o ciclo de vida de desenvolvimento dos modelos.

% Código para centralizar uma figura e posicioná-la "exatamente" no texto
% O opção 'h' posiciona a imagem
% \centering dentro do ambiente figure centraliza
% \begin{figure}[h]
%   \centering 
%   \includegraphics[scale=0.5]{images_generated_with_text_three_engineers.png}
%   \caption{Imagem gerada a partir do texto "Three engineers running on the grassland" by Stable Diffusion v2.1.}
%   \label{fig:images_of_three_engineers}
% \end{figure} 

\begin{wrapfigure}{r}{0.50\textwidth}
  \centering 
  \includegraphics[scale=0.47]{images_generated_with_text_three_engineers.png}
  \caption{Imagem gerada a partir do texto ``Three engineers running on the grassland'' \space  by Stable Diffusion v2.1.}
  \label{fig:images_of_three_engineers}
\end{wrapfigure}


% \begin{wrapfigure}{l}{0.25\textwidth}
%     \centering
%     \includegraphics[width=0.25\textwidth]{contour}
% \end{wrapfigure}

À medida que as técnicas avançam surgem situações de violação de direitos autorais
com os conteúdos gerados artificialmente. Um dos desafios é associar a legislação da propriedade intelectual, 
que define de forma clara o que é um trabalho original, a quando o direito à propriedade intelectual é violado. Por isso, a definição de propriedade intelectual no âmbito dos modelos de geração de conteúdos por IA permanece obscura e indefinida. Alguns exemplos de problemas surgem em modelos geradores baseados em texto-para-imagem, acusados de infringir os trabalhos de artistas, consequência do uso de grandes volumes de imagens da internet para o treinamento dos modelos como Stable Diffusion.





Uma estratégia de minimizar a violação da propriedade intelectual é permitir que os artistas que considerarem seus direitos autorais violados tenham seus trabalhos removidos dos datasets, como a empresa Midjourney. 
Outras companhias, como Stability AI, planejam permitir que os próprios artistas removam seus trabalhos, 
dando mais flexibilidade ao processo de proteção intelectual. Para textos a marca d'água têm sido utilizada auxiliando na verificação do uso de fontes sem permissão para a geração de conteúdos. A companhia OpenAI trabalha para disponibilizar um classificador que consiga diferenciar entre texto gerado por modelos de IA e textos escritos por pessoas.


Após breve análise dos principais elementos desejáveis no escopo de GCIA responsáveis, faz-se necessário avaliar outras característivas que devem existir nos modelos.

Os conteúdos gerados pelos modelos podem propagar informações danosas e falsas, em que os resultados são difíceis de serem diferenciados de conteúdos criados por pessoas. Além disso, o uso para fins comerciais gera bastante controvérsia. A possibilidade da substituição, de forma descontrolada, de trabalhos realizados por pessoas é desconfortável para muitos.

Os modelos podem ser vistos como caixas-preta. Isto leva a uma necessidade de se explicar como o resultado é criado e suas bases. Disponibilizar o código-fonte desses modelos pode propagar as falhas já mencionadas nos novos modelos, com impactos imprevisíveis. A possibilidade dos usuários fornecerem \textit{feedback} sobre o uso dos modelos é necessário para serem responsáveis. Como exemplo, a OpenAI possibilita que usuários enviem feedback que são utilizados para a correção e melhoria, reduzindo impactos negativos. 

Frequentemente os modelos são treinados com dados sem o devido consentimento, desrespeitando os direitos autorais e sem compensação aos autores pelo uso dos dados. Para evitar esse problema, as companhias passaram a agir de forma ativa junto aos proprietários dos dados antes de realizarem o treinamento de seus modelos. Um caminho para contornar essa situação seria as empresas procurarem os autores solicitando autorização para uso dos dados bem como recompensá-los quando os dados forem consultados. Outro impacto é o elevado consumo de energia elétrica pelos computadores para treinar os modelos cada vez maiores. Os modelos devem buscar o equilíbrio de benefícios evitando que pessoas sejam privilegiadas em detrimento de outros grupos.


Existem conflitos entre os objetivos de realização dos modelos de IA. Ao focar em prevenir certos riscos, como uso de termos tóxicos em modelos de linguagem, expõem-se outras vulnerabilidades e riscos, como a introdução e manutenção de viéses na predição. Neste exemplo, isso pode acarretar na discriminação de comunidades.

O artigo indica que GCIA está em crescente e rápida evolução, com avanços impressionantes nas artes mas com vários riscos em seu uso. Resume atuais e potenciais riscos do uso de modelos possibilitando que empresas e usuários previnam-se, propondo ações para minimizá-los. Conhecer, discutir, medir e agir sobre os riscos é um bom caminho para o futuro dos modelos responsáveis.
